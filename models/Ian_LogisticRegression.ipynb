{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source: \n",
    "https://www.analyticsvidhya.com/blog/2020/01/link-prediction-how-to-predict-your-future-connections-on-facebook/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sampling library : \n",
    "    https://github.com/benedekrozemberczki/littleballoffur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import all the necessary libraries and modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import networkx as nx\n",
    "from tqdm import tqdm\n",
    "#import re\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "from littleballoffur import MetropolisHastingsRandomWalkSampler #-> this library here\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000\n"
     ]
    }
   ],
   "source": [
    "# load nodes details\n",
    "with open(\"train.txt\") as f:\n",
    "    reader = csv.reader(f, delimiter = \"\\t\")\n",
    "    train_nodes = list(reader)\n",
    "print(len(train_nodes))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydict = {}\n",
    "for node_line in train_nodes:\n",
    "    key = node_line.pop(0)\n",
    "    mydict[key] = node_line\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#form our source and target nodes list\n",
    "node_list1 = []\n",
    "node_list2 = []\n",
    "for key,val in mydict.items():\n",
    "    for i in range(len(val)):\n",
    "        node_list1.append(key)\n",
    "        node_list2.append(val[i])\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 183. MiB for an array with shape (24004361,) and data type int64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-39f13c126c46>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnetwork_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'source'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mnode_list1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'sink'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mnode_list2\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mnetwork_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    465\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    466\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 467\u001b[1;33m             \u001b[0mmgr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minit_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    468\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMaskedArray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    469\u001b[0m             \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmrecords\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmrecords\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36minit_dict\u001b[1;34m(data, index, columns, dtype)\u001b[0m\n\u001b[0;32m    281\u001b[0m             \u001b[0marr\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_datetime64tz_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0marr\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    282\u001b[0m         ]\n\u001b[1;32m--> 283\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marrays_to_mgr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    284\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    285\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36marrays_to_mgr\u001b[1;34m(arrays, arr_names, index, columns, dtype, verify_integrity)\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m         \u001b[1;31m# don't force copy because getting jammed in an ndarray anyway\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 83\u001b[1;33m         \u001b[0marrays\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_homogenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m         \u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mensure_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36m_homogenize\u001b[1;34m(data, index, dtype)\u001b[0m\n\u001b[0;32m    350\u001b[0m                 \u001b[0mval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfast_multiget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    351\u001b[0m             val = sanitize_array(\n\u001b[1;32m--> 352\u001b[1;33m                 \u001b[0mval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraise_cast_failure\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    353\u001b[0m             )\n\u001b[0;32m    354\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\construction.py\u001b[0m in \u001b[0;36msanitize_array\u001b[1;34m(data, index, dtype, copy, raise_cast_failure)\u001b[0m\n\u001b[0;32m    441\u001b[0m             \u001b[0msubarr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_try_cast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraise_cast_failure\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 443\u001b[1;33m             \u001b[0msubarr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmaybe_convert_platform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    444\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    445\u001b[0m         \u001b[0msubarr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmaybe_cast_to_datetime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubarr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\dtypes\\cast.py\u001b[0m in \u001b[0;36mmaybe_convert_platform\u001b[1;34m(values)\u001b[0m\n\u001b[0;32m     92\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"_values\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 94\u001b[1;33m         \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmaybe_convert_objects\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.maybe_convert_objects\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 183. MiB for an array with shape (24004361,) and data type int64"
     ]
    }
   ],
   "source": [
    "network_df = pd.DataFrame({'source':node_list1, 'sink':node_list2})\n",
    "network_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a list of tuples where each tuple represent an edge\n",
    "node_list = list(zip(network_df['source'],network_df['sink']))\n",
    "#remove duplicates from list\n",
    "node_list = list(set([i for i in lst])) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_complete = nx.DiGraph()\n",
    "G_complete.add_edges_from(node_list)\n",
    "\n",
    "#Metropolis Hastings Sampling using littleballoffur library\n",
    "from littleballoffur import MetropolisHastingsRandomWalkSampler\n",
    "\n",
    "sampler = MetropolisHastingsRandomWalkSampler()\n",
    "\n",
    "#output is a NetworkX graph of sampled edges\n",
    "G = sampler.sample(G_complete)\n",
    "\n",
    "#unzip the pairs into source and sink\n",
    "source_sink = list(zip(*G.edges()))\n",
    "\n",
    "#create a dataframe with the sampled edges\n",
    "network_df_sample = pd.DataFrame({\"source\":source_sink[0], \"sink\":source_sink[1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Sampling (Either run this or the above cell)\n",
    "#network_df_sample = network_df.sample(frac = 0.0001)\n",
    "#network_df_sample.shape\n",
    "#G = nx.from_pandas_edgelist(network_df_sample,source='source',target='sink', edge_attr=None, create_using=nx.DiGraph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot graph\n",
    "\n",
    "plt.figure(figsize=(20,20))\n",
    "pos = nx.random_layout(G, seed=23)\n",
    "nx.draw(G, with_labels=False,  pos = pos, node_size = 40, alpha = 0.6, width = 0.7, connectionstyle='arc3, rad = 0.1')\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# combine all nodes in a list\n",
    "node_list = node_list1_sampled + node_list2_sampled\n",
    "\n",
    "# remove duplicate items from the list\n",
    "node_list = sorted(list(dict.fromkeys(node_list)))\n",
    "\n",
    "# build adjacency matrix\n",
    "adj_G = nx.to_numpy_matrix(G, node_list)\n",
    "adj_G.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this block, we find all the unconnected pairs and assign the 'link' label as zero\n",
    "#find unconnected pairs\n",
    "gEdges = G.edges()\n",
    "unconnected_pairs = set()\n",
    "for a in G.nodes():\n",
    "    for b in G.nodes():\n",
    "        if a != b and (a,b) not in gEdges:\n",
    "            unconnected_pairs.add( (a, b) )\n",
    "\n",
    "#set -> list\n",
    "for pairs in unconnected_pairs:\n",
    "    pairs = list(pairs)\n",
    "    \n",
    "unconnected_pairs = list(unconnected_pairs)\n",
    "\n",
    "#create negative samples aka. samples with link = 0\n",
    "node_1_unlinked = [i[0] for i in unconnected_pairs]\n",
    "node_2_unlinked = [i[1] for i in unconnected_pairs]\n",
    "\n",
    "data = pd.DataFrame({'source':node_1_unlinked, \n",
    "                     'sink':node_2_unlinked})\n",
    "\n",
    "# add target variable 'link'\n",
    "data['link'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In this block, we find all the droppable pairs and assign the 'link' label as one, this represents our positive samples\n",
    "#Remove Links from Connected Node Pairs – Positive Samples\n",
    "# make sure that in the process of dropping edges, all the nodes of the graph should remain connected.\n",
    "initial_node_count = len(G.nodes)\n",
    "network_df_temp = network_df_sample.copy()\n",
    "\n",
    "# empty list to store removable links\n",
    "omissible_links_index = []\n",
    "\n",
    "for i in tqdm(network_df_sample.index.values):\n",
    "  \n",
    "  # remove a node pair and build a new graph\n",
    "  G_temp = nx.from_pandas_edgelist(network_df_temp.drop(index = i), \"source\", \"sink\", create_using=nx.DiGraph())\n",
    "  \n",
    "  # check there is no spliting of graph and number of nodes is same\n",
    "  if (nx.number_weakly_connected_components(G_temp) == 1) and (len(G_temp.nodes) == initial_node_count):\n",
    "    omissible_links_index.append(i)\n",
    "    network_df_temp = network_df_temp.drop(index = i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(omissible_links_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe of removable edges\n",
    "network_df_ghost = network_df_sample.loc[omissible_links_index]\n",
    "\n",
    "# add the target variable 'link'\n",
    "network_df_ghost['link'] = 1\n",
    "\n",
    "# Remember data already contains edges labeled '0', now we add edges labeled '1'\n",
    "data = data.append(network_df_ghost[['source', 'sink', 'link']], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['link'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop removable edges\n",
    "network_df_sample_partial = network_df_sample.drop(index=network_df_ghost.index.values)\n",
    "\n",
    "# create a new graph after dropping the removable links\n",
    "G_data = nx.from_pandas_edgelist(network_df_sample_partial, \"source\", \"sink\", create_using=nx.DiGraph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate walks\n",
    "node2vec = Node2Vec(G_data, dimensions=100, walk_length=16, num_walks=50)\n",
    "\n",
    "# train node2vec model\n",
    "n2w_model = node2vec.fit(window=7, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply the trained node2vec model on each and every node pair in the dataframe ‘data’\n",
    "x = [(n2w_model[str(i)]+n2w_model[str(j)]) for i,j in zip(data['source'], data['sink'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Validate performance of model\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(np.array(x), data['link'], \n",
    "                                                test_size = 0.3, \n",
    "                                                random_state = 35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train and predict a logistic regression model\n",
    "lr = LogisticRegression(class_weight=\"balanced\")\n",
    "\n",
    "lr.fit(xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = lr.predict_proba(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(ytest, predictions[:,1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
