{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source\": \n",
    "https://www.analyticsvidhya.com/blog/2020/01/link-prediction-how-to-predict-your-future-connections-on-facebook/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import all the necessary libraries and modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import tqdm\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from node2vec import Node2Vec\n",
    "from sklearn.cluster import KMeans\n",
    "from node2vec.edges import HadamardEmbedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24004361, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>sink</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>540762</td>\n",
       "      <td>1912140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>540762</td>\n",
       "      <td>1537559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>540762</td>\n",
       "      <td>3091331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>540762</td>\n",
       "      <td>2757277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>540762</td>\n",
       "      <td>3237295</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   source     sink\n",
       "0  540762  1912140\n",
       "1  540762  1537559\n",
       "2  540762  3091331\n",
       "3  540762  2757277\n",
       "4  540762  3237295"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load nodes details\n",
    "with open(\"data/train.txt\") as f:\n",
    "    reader = csv.reader(f, delimiter = \"\\t\")\n",
    "    train_nodes = list(reader)\n",
    "\n",
    "#create a dict with the source as key as the sinks as values\n",
    "mydict = {}\n",
    "for node_line in train_nodes:\n",
    "    key = node_line.pop(0)\n",
    "    mydict[key] = node_line\n",
    "\n",
    "#form our source and target nodes list\n",
    "node_list1 = []\n",
    "node_list2 = []\n",
    "for key,val in mydict.items():\n",
    "    for i in range(len(val)):\n",
    "        node_list1.append(key)\n",
    "        node_list2.append(val[i])\n",
    "        \n",
    "#display dataframe\n",
    "network_df = pd.DataFrame({'source':node_list1, 'sink':node_list2})\n",
    "print(network_df.shape)\n",
    "network_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_list = list(zip(network_df['source'],network_df['sink']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_complete = nx.DiGraph()\n",
    "G_complete.add_edges_from(node_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4867136\n"
     ]
    }
   ],
   "source": [
    "print(len(G_complete.nodes()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Randomly sample 0.1% of our entire edge pairs\n",
    "network_df_sample = network_df.sample(frac = 0.0001).reset_index(drop = True)\n",
    "network_df_sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_df_sample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "node_list1_sampled = list(network_df_sample['source'])\n",
    "node_list2_sampled = list(network_df_sample['sink'])\n",
    "node_pairs = list(zip(node_list1_sampled,node_list2_sampled))\n",
    "print(node_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a graph for our sampled pairs\n",
    "G = nx.from_pandas_edgelist(network_df_sample,source='source',target='sink', edge_attr=None, create_using=nx.DiGraph())\n",
    "\n",
    "plt.figure(figsize=(20,20))\n",
    "pos = nx.random_layout(G, seed=23)\n",
    "nx.draw(G, with_labels=False,  pos = pos, node_size = 40, alpha = 0.6, width = 0.7, connectionstyle='arc3, rad = 0.1')\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# combine all nodes in a list\n",
    "node_list = node_list1_sampled + node_list2_sampled\n",
    "\n",
    "# remove duplicate items from the list\n",
    "node_list = list(dict.fromkeys(node_list))\n",
    "\n",
    "# build adjacency matrix\n",
    "adj_G = nx.to_numpy_matrix(G, node_list)\n",
    "adj_G.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate walks\n",
    "node2vec = Node2Vec(G, dimensions=2, walk_length=20, num_walks=10,workers=4)\n",
    "# Learn node embeddings \n",
    "model = node2vec.fit(window=10, min_count=1)\n",
    "model.wv.save_word2vec_format(\"embedding.emb\") #save the embedding in file embedding.emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"word2vec.model\") #save the model for later use \n",
    "#use model = Word2Vec.load(\"word2vec.model\") to load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('137924', 1.0),\n",
       " ('2150273', 0.9999995231628418),\n",
       " ('461343', 0.9999979734420776),\n",
       " ('4010763', 0.9999911785125732),\n",
       " ('4093673', 0.9999781250953674),\n",
       " ('1183655', 0.9999607801437378),\n",
       " ('1925933', 0.999956488609314),\n",
       " ('2934751', 0.9999469518661499),\n",
       " ('2036173', 0.999945878982544),\n",
       " ('1051108', 0.9999414682388306)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#example: find most similar nodes to node 1705425\n",
    "model.wv.most_similar('3361377')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating edge features: 100%|█████████████████████████████████████████| 6249880/6249880.0 [01:43<00:00, 60488.56it/s]\n"
     ]
    }
   ],
   "source": [
    "# Learn link embeddings\n",
    "edges_embs = HadamardEmbedder(keyed_vectors=model.wv)\n",
    "edges_kv = edges_embs.as_keyed_vectors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save embeddings for later use\n",
    "edges_kv.save_word2vec_format('edgeembedding.emb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#both outputs an array\n",
    "#model.wv.get_vector('2')\n",
    "#edges_embs[('1', '2')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"word '('1705425', '3006327')' not in vocabulary\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-05bd4d561fe4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#example: find most similar edges\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0medges_kv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'1705425'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'3006327'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mmost_similar\u001b[1;34m(self, positive, negative, topn, restrict_vocab, indexer)\u001b[0m\n\u001b[0;32m    551\u001b[0m                 \u001b[0mmean\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 553\u001b[1;33m                 \u001b[0mmean\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_norm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    554\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    555\u001b[0m                     \u001b[0mall_words\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mword_vec\u001b[1;34m(self, word, use_norm)\u001b[0m\n\u001b[0;32m    466\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    467\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 468\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"word '%s' not in vocabulary\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    469\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    470\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"word '('1705425', '3006327')' not in vocabulary\""
     ]
    }
   ],
   "source": [
    "#example: find most similar edges\n",
    "edges_kv.most_similar(str(('1705425', '3006327')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.loadtxt(\"embedding.emb\", skiprows=1) # load the embedding of the nodes of the graph\n",
    "print(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort the embedding based on node index in the first column in X\n",
    "X=X[X[:,0].argsort()]; \n",
    "print(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z=X[0:X.shape[0],1:X.shape[1]]; # remove the node index from X and save in Z\n",
    "\n",
    "kmeans = KMeans(n_clusters=2, random_state=0).fit(Z) # apply kmeans on Z\n",
    "labels=kmeans.labels_  # get the cluster labels of the nodes.\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get unconnected node-pairs\n",
    "all_unconnected_pairs = []\n",
    "\n",
    "# traverse adjacency matrix\n",
    "offset = 0\n",
    "for i in (range(adj_G.shape[0])):\n",
    "    for j in range(offset,adj_G.shape[1]):\n",
    "        if i != j: #if not self\n",
    "            #if nx.shortest_path_length(G, str(i), str(j)) <=2: \n",
    "            if adj_G[i,j] == 0:\n",
    "                  all_unconnected_pairs.append([node_list1_sampled[i],node_list2_sampled[j]])\n",
    "\n",
    "    offset = offset + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#find unconnected pairs\n",
    "gEdges = G.edges()\n",
    "unconnected_pairs = set()\n",
    "for a in G.nodes():\n",
    "    for b in G.nodes():\n",
    "        if a != b and (a,b) not in gEdges:\n",
    "            unconnected_pairs.add( (a, b) )\n",
    "\n",
    "for pairs in unconnected_pairs:\n",
    "    pairs = list(pairs)\n",
    "    \n",
    "unconnected_pairs = list(unconnected_pairs)\n",
    "\n",
    "#create negative samples\n",
    "node_1_unlinked = [i[0] for i in unconnected_pairs]\n",
    "node_2_unlinked = [i[1] for i in unconnected_pairs]\n",
    "\n",
    "data = pd.DataFrame({'node_1':node_1_unlinked, \n",
    "                     'node_2':node_2_unlinked})\n",
    "\n",
    "# add target variable 'link'\n",
    "data['link'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Graphs with Graph Convolutional Networks\n",
    "order = sorted(list(G.nodes()))\n",
    "A = nx.to_numpy_matrix(G, nodelist=order)\n",
    "\n",
    "I = np.eye(G.number_of_nodes())\n",
    "A_hat = A + I\n",
    "D_hat = np.array(np.sum(A_hat, axis=0))[0]\n",
    "D_hat = np.matrix(np.diag(D_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W_1 = np.random.normal(\n",
    "    loc=0, scale=1, size=(G.number_of_nodes(), 4))\n",
    "W_2 = np.random.normal(\n",
    "    loc=0, size=(W_1.shape[1], 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Stack the GCN layers. We here use just the identity matrix as feature representation, aka.\n",
    "#each node is represented as a one-hot encoded categorical variable.\n",
    "def relu(X):\n",
    "    return np.maximum(0,X)\n",
    "\n",
    "def gcn_layer(A_hat, D_hat, X, W):\n",
    "    return relu(D_hat**-1 * A_hat * X * W)\n",
    "\n",
    "H_1 = gcn_layer(A_hat, D_hat, I, W_1)\n",
    "H_2 = gcn_layer(A_hat, D_hat, H_1, W_2)\n",
    "output = H_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "feature_representations = {int(node): np.array(output)[int(node)] for node in G.nodes()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "feature_representations = {}\n",
    "ct = 0\n",
    "for node in order:\n",
    "    feature_representations[int(node)]=np.array(output)[ct]\n",
    "    ct +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "xs,ys = zip(*feature_representations.values())\n",
    "labels = feature_representations.keys()   \n",
    "\n",
    "# display\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.title('Scatter Plot', fontsize=20)\n",
    "plt.xlabel('x', fontsize=15)\n",
    "plt.ylabel('y', fontsize=15)\n",
    "plt.scatter(xs, ys, marker = 'o')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Different from above\n",
    "initial_node_count = len(G.nodes)\n",
    "network_df_temp = network_df.copy()\n",
    "\n",
    "# empty list to store removable links\n",
    "omissible_links_index = []\n",
    "\n",
    "for i in network_df.index.values:\n",
    "  \n",
    "  # remove a node pair and build a new graph\n",
    "  G_temp = nx.from_pandas_edgelist(network_df_temp.drop(index = i), \"source\", \"sink\", create_using=nx.MultiGraph())\n",
    "  \n",
    "  # check there is no spliting of graph and number of nodes is same\n",
    "  if (nx.number_connected_components(G_temp) == 1) and (len(G_temp.nodes) == initial_node_count):\n",
    "    omissible_links_index.append(i)\n",
    "    network_df_temp = network_df_temp.drop(index = i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
